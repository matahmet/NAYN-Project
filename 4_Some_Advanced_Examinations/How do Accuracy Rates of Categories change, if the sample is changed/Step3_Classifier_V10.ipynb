{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 1st\n",
    "#upload raw data\n",
    "import pandas as pd\n",
    "raw_data = pd.read_csv(\"https://raw.githubusercontent.com/naynco/nayn.data/master/classification_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case URL is dead\n",
    "#Back-up for for raw_data\n",
    "#import pickle\n",
    "#raw_data.to_pickle(\"./raw_data.pkl\")\n",
    "#For backing up saved raw_data\n",
    "#with open(\"raw_data.pkl\", \"rb\") as fp:\n",
    "#    raw_data= pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12006</th>\n",
       "      <td>58 Saniyede Katar Meselesi? Katar krizi nedir?...</td>\n",
       "      <td>DÜNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>58 Saniyede Türkiye - Almanya Gerginliği</td>\n",
       "      <td>DÜNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12877</th>\n",
       "      <td>Adriana Lima, Bomba Aşkla İlgili İlk Kez Konuş...</td>\n",
       "      <td>DÜNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12878</th>\n",
       "      <td>Galatasaraylı Taraftarlar Patladı: İstifa Edin</td>\n",
       "      <td>SPOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12880</th>\n",
       "      <td>Galatasaray'dan Ayrılan Sabri, Neredeyse Bedav...</td>\n",
       "      <td>SPOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title Categories\n",
       "12006  58 Saniyede Katar Meselesi? Katar krizi nedir?...      DÜNYA\n",
       "12496           58 Saniyede Türkiye - Almanya Gerginliği      DÜNYA\n",
       "12877  Adriana Lima, Bomba Aşkla İlgili İlk Kez Konuş...      DÜNYA\n",
       "12878     Galatasaraylı Taraftarlar Patladı: İstifa Edin       SPOR\n",
       "12880  Galatasaray'dan Ayrılan Sabri, Neredeyse Bedav...       SPOR"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#block 2nd\n",
    "\n",
    "#learn size of raw_data\n",
    "#print(len(raw_data))\n",
    "#learn unique values in Categories column\n",
    "##raw_data[\"Categories\"].unique()\n",
    "#tackled categories\n",
    "main_categories = ['DÜNYA', 'SPOR','SANAT','Teknoloji']\n",
    "#get filter\n",
    "filter=raw_data[\"Categories\"].isin(main_categories)\n",
    "#apply filter\n",
    "filtered_data=raw_data[filter]\n",
    "#learn size of filtered_data\n",
    "#print(len(filtered_data))\n",
    "##filtered_data.head()\n",
    "#remove redundant columns\n",
    "tackled_data0=filtered_data[['Title','Categories']]\n",
    "tackled_data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12006</th>\n",
       "      <td>58 saniyede katar meselesi? katar krizi nedir?...</td>\n",
       "      <td>DÜNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>58 saniyede türkiye - almanya gerginliği</td>\n",
       "      <td>DÜNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12877</th>\n",
       "      <td>adriana lima, bomba aşkla ilgili ilk kez konuş...</td>\n",
       "      <td>DÜNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12878</th>\n",
       "      <td>galatasaraylı taraftarlar patladı: istifa edin</td>\n",
       "      <td>SPOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12880</th>\n",
       "      <td>galatasaray'dan ayrılan sabri, neredeyse bedav...</td>\n",
       "      <td>SPOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title Categories\n",
       "12006  58 saniyede katar meselesi? katar krizi nedir?...      DÜNYA\n",
       "12496           58 saniyede türkiye - almanya gerginliği      DÜNYA\n",
       "12877  adriana lima, bomba aşkla ilgili ilk kez konuş...      DÜNYA\n",
       "12878     galatasaraylı taraftarlar patladı: istifa edin       SPOR\n",
       "12880  galatasaray'dan ayrılan sabri, neredeyse bedav...       SPOR"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#block 3rd\n",
    "\n",
    "#method-just lower case as a transformation\n",
    "from unicode_tr import unicode_tr\n",
    "# just lower case\n",
    "def just_lower(doc):\n",
    "    doc=unicode_tr(doc).lower()\n",
    "    return doc\n",
    "tackled_data = tackled_data0.copy()\n",
    "#\n",
    "tackled_data['Title'] = tackled_data0['Title'].apply(just_lower)\n",
    "y_general=tackled_data[\"Categories\"]\n",
    "\n",
    "#tackled_data[tackled_data.index==14252]\n",
    "tackled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 4th\n",
    "#split train-test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tackled_data['Title'],tackled_data['Categories'],test_size=0.2,random_state=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 5th\n",
    "#save y_test_57\n",
    "y_test.to_pickle(\"./y_test_57.pkl\")\n",
    "\n",
    "#save y_train_57\n",
    "y_train.to_pickle(\"./y_train_57.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 6th\n",
    "#get distribution of categories in general, train and test set, respectively\n",
    "general_counts=[0,0,0,0]\n",
    "general_percents=[.0,.0,.0,.0]\n",
    "train_counts=[0,0,0,0]\n",
    "train_percents=[.0,.0,.0,.0]\n",
    "test_counts=[0,0,0,0]\n",
    "test_percents=[.0,.0,.0,.0]\n",
    "general_count=len(y_general)\n",
    "train_count=len(y_train)\n",
    "test_count=len(y_test)\n",
    "for i in range(0,4):\n",
    "    general_counts[i]=y_general[y_general==main_categories[i]].count()\n",
    "    general_percents[i]=round(general_counts[i]/general_count,2)\n",
    "    train_counts[i]=y_train[y_train==main_categories[i]].count()\n",
    "    train_percents[i]=round(train_counts[i]/train_count,2)\n",
    "    test_counts[i]=y_test[y_test==main_categories[i]].count()\n",
    "    test_percents[i]=round(test_counts[i]/test_count,2)\n",
    "    \n",
    "#print(\"summary\")\n",
    "#print(main_categories)\n",
    "#print(\"for general\")\n",
    "#print(general_counts) \n",
    "#print(general_percents) \n",
    "#print(\"for train\")\n",
    "#print(train_counts) \n",
    "#print(train_percents)\n",
    "#print(\"for test\")\n",
    "#print(test_counts) \n",
    "#print(test_percents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 7th\n",
    "#Primitive sub_methods for analysis\n",
    "# the method saying whether word is in document\n",
    "def is_inside(word,doc):\n",
    "    if doc.find(word)==-1:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "# the method saying whether word list is in document together    \n",
    "def is_inside_list(word_list,doc):\n",
    "    result=True\n",
    "    for word in word_list:\n",
    "        result=(result and is_inside(word,doc))\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 8th\n",
    "#test for methods in block 7th\n",
    "#from first part of poland anthem\n",
    "#print(is_inside(\"żyjemy\",\"Jeszcze Polska nie zginęła,Kiedy my żyjemy.Co nam obca przemoc wzięła,Szablą odbierzemy.\"))\n",
    "#from first part of estonian anthem\n",
    "#print(is_inside_list([\"kaunis\",\"isamaa\"],\"Mu isamaa, mu õnn ja rõõm,kui kaunis oled sa!Ei leia mina iial teal\"))\n",
    "#from first part of poland anthem\n",
    "#print(is_inside(\"Przejdziem\",\"Jeszcze Polska nie zginęła,Kiedy my żyjemy.Co nam obca przemoc wzięła,Szablą odbierzemy.\"))\n",
    "#from first part of estonian anthem\n",
    "#print(is_inside_list([\"kaunis\",\"truuiks\"],\"Mu isamaa, mu õnn ja rõõm,kui kaunis oled sa!Ei leia mina iial teal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 9th\n",
    "#analyze methods\n",
    "#analyze_word, get weights to indicate tendencies of a word over categories\n",
    "def analyze_word(word,train_X,train_y):\n",
    "    #initialization of output, count of word in docs with category\n",
    "    category_counts=[0,0,0,0]\n",
    "    #iterate over train_X\n",
    "    for i in train_X.index:\n",
    "            doc=train_X[i]\n",
    "            #word in doc ?\n",
    "            if is_inside(word,doc):\n",
    "                #if yes, what category of the doc?\n",
    "                for j in range(0,4):\n",
    "                    #if yes, increment by 1 on count of related category\n",
    "                    if train_y[i]==main_categories[j]:\n",
    "                        category_counts[j]=category_counts[j]+1\n",
    "                \n",
    "                 \n",
    "    #sum of counts of number of category                   \n",
    "    sum_counts=sum(category_counts)\n",
    "    #if there is no category related to the word, then no prediction\n",
    "    if sum_counts==0:\n",
    "            sum_counts=1\n",
    "            label1=\"No Prediction\"\n",
    "            label2=\"No Prediction\"\n",
    "    else:\n",
    "        temp=category_counts.copy()\n",
    "        #get category having the largest count of word on docs\n",
    "        label1=main_categories[temp.index(max(temp))]\n",
    "        #get category having 2nd largest count of word on docs\n",
    "        #(1)set 0 for category having the largest count of word on docs\n",
    "        temp[temp.index(max(temp))]=0\n",
    "        #(2)thereby,category of largest count of temp after setting is category having 2nd largest count of word on docs\n",
    "        label2=main_categories[temp.index(max(temp))]\n",
    "        #if there is no 2nd largest count of word on docs\n",
    "        if max(temp)==0:\n",
    "            label2=\"No Prediction\"\n",
    "        else:\n",
    "            label2=main_categories[temp.index(max(temp))]\n",
    "     \n",
    "    #get rates, count by sum of counts, as probabilities of categories over word \n",
    "    category_probs = [x / sum_counts for x in category_counts]\n",
    "    #round to two digits\n",
    "    category_probs=[round(x,2) for x in category_probs ]\n",
    "    #at last outputs: the word, lengths of the word, category counts, label1 ans label2 \n",
    "    return [word,len(word),category_counts,category_probs,label1,label2]\n",
    "\n",
    "#analyze_wordlist, get weights to indicate tendencies of elements of wordlist over categories\n",
    "def analyze_wordlist(word_list,train_X,train_y):\n",
    "   #initialization of output, count of word in docs with category\n",
    "    category_counts=[0,0,0,0]\n",
    "    #iterate over train_X\n",
    "    for i in train_X.index:\n",
    "            doc=train_X[i]\n",
    "            #elements of word_list in doc ?\n",
    "            if is_inside_list(word_list,doc):               \n",
    "                for j in range(0,4):\n",
    "                    #if yes, increment by 1 on count of related category\n",
    "                    if train_y[i]==main_categories[j]:                        \n",
    "                        category_counts[j]=category_counts[j]+1\n",
    "                       \n",
    "               \n",
    "    #sum of counts of number of category                   \n",
    "    sum_counts=sum(category_counts)\n",
    "    #if there is no category related to the word, then no prediction\n",
    "    if sum_counts==0:\n",
    "            sum_counts=1\n",
    "            label1=\"No Prediction\"\n",
    "            label2=\"No Prediction\"\n",
    "    else:\n",
    "        temp=category_counts.copy()\n",
    "        #get category having the largest count of word on docs\n",
    "        label1=main_categories[temp.index(max(temp))]\n",
    "        #get category having 2nd largest count of word on docs\n",
    "        #(1)set 0 for category having the largest count of word on docs\n",
    "        temp[temp.index(max(temp))]=0\n",
    "        #(2)thereby,category of largest count of temp after setting is category having 2nd largest count of word on docs\n",
    "        label2=main_categories[temp.index(max(temp))]\n",
    "        #if there is no 2nd largest count of word on docs\n",
    "        if max(temp)==0:\n",
    "            label2=\"No Prediction\"\n",
    "        else:\n",
    "            label2=main_categories[temp.index(max(temp))]\n",
    "    #get rates, count by sum of counts, as probabilities of categories over word \n",
    "    category_probs = [x / sum_counts for x in category_counts]\n",
    "    #round to two digits\n",
    "    category_probs=[round(x,2) for x in category_probs ]\n",
    "    #at last outputs: word_list, sum of lengths of words in wordlist, category counts, label1 ans label2 \n",
    "    return [word_list,len(''.join(word_list)),category_counts,category_probs,label1,label2]\n",
    "\n",
    "#extended(detailed) analyze methods in case of need\n",
    "#%run ./extended_analysis_methods.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 10th\n",
    "#test for analyze_word\n",
    "#print(analyze_word(\"finans\",X_train,y_train))\n",
    "#print(analyze_word(\"galatasaray\",X_train,y_train))\n",
    "#print(analyze_word(\"matmazel\",X_train,y_train))\n",
    "#print(analyze_word(\"oscar\",X_train,y_train))\n",
    "#print(analyze_word(\"filistin\",X_train,y_train))\n",
    "#print(analyze_word(\"israil\",X_train,y_train))\n",
    "#print(analyze_word(\"dolar\",X_train,y_train))\n",
    "#print(analyze_word(\"euro\",X_train,y_train))\n",
    "#test for analyze_wordlist\n",
    "#print(analyze_wordlist([\"filistin\",\"israil\"],X_train,y_train))\n",
    "#print(analyze_wordlist([\"filistin\",\"israil\",\"abd\"],X_train,y_train))\n",
    "#print(analyze_wordlist([\"oscar\",\"oyuncu\"],X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 9th\n",
    "#import preprocess methods\n",
    "%run ./Step2_preprocess_word.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 10th\n",
    "#test preprocess method\n",
    "#import random\n",
    "#n=random.randint(1,test_count)\n",
    "#print(n)\n",
    "#doc=X_test[X_test.index[n]]\n",
    "#label=y_test[X_test.index[n]]\n",
    "#print(\"doc:\",doc)\n",
    "#print(\"label:\",label)\n",
    "#print(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 11th\n",
    "\n",
    "#analyze methods for doc (string)\n",
    "\n",
    "# one word-based\n",
    "\n",
    "def analyze_doc(doc,train_X,train_y):\n",
    "    #get stems with maximum length of words in doc\n",
    "    max_stems=preprocess(doc)\n",
    "    results=[]\n",
    "    for word in max_stems:\n",
    "        #analyze for each word in doc\n",
    "        data=analyze_word(word,train_X,train_y)\n",
    "        #if max_stems in docs of X_train\n",
    "        if sum(data[2])!=0:\n",
    "            #append to results\n",
    "            results.append(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return results\n",
    "\n",
    "# wordlist with n-word selecting by combinational approach\n",
    "#in order to select stems in doc, we use a method based on combination\n",
    "#for example number of elements in list of preprocess(doc) is m=7, and you choose a combination with n=2 elements\n",
    "#thereby you have C(7,2)=(7*6)/2=21 combinations as a wordlist with 2 elements.\n",
    "#method gives combination as wordlist having tendencies over categories\n",
    "\n",
    "import itertools\n",
    "\n",
    "def analyze_doc_s(doc,train_X,train_y,s):\n",
    "    #get stems with maximum length of words in doc\n",
    "    max_stems=preprocess(doc)\n",
    "    #n, number of combination\n",
    "    #len(max_stems) is number of element of preprocess(doc)\n",
    "    # of course, if n>m, C(m,s) is not meaningful\n",
    "    if s>len(max_stems):\n",
    "        return []\n",
    "    results=[]\n",
    "    #s_list is list of combination with s element of preprocess(doc) or max_stems \n",
    "    s_list=list(set(itertools.combinations(max_stems, s)))\n",
    "    for word_list in s_list:\n",
    "        #for each combination as a wordlist, get result of method analyze_wordlist\n",
    "        word_list=list(word_list)\n",
    "        data=analyze_wordlist(word_list,train_X,train_y)\n",
    "        #if that combination has at least one category, append the results\n",
    "        # doc in test_set by default\n",
    "        if sum(data[2])>0:\n",
    "            results.append(data)\n",
    "        \n",
    "        #if doc in train_set\n",
    "        # sum of count of categories must be greater than 1, because we want another documents involving that combination\n",
    "        #if sum(data[2])>1:\n",
    "            #results.append(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 12th\n",
    "#doc defined in block 10th\n",
    "#test of analyze_doc\n",
    "#print(\"doc:\",doc)\n",
    "#print(\"label:\",label)\n",
    "#analyze_doc(doc,X_train,y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 13th\n",
    "\n",
    "#doc defined in block 10th\n",
    "#test of analyze_doc_n\n",
    "#analyze_doc_n(doc,X_train,y_train,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
